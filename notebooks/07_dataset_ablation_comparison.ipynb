{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from dapinet.analysis import (\n",
    "    load_datasets,\n",
    "    run_model_inference,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variants to evaluate\n",
    "# Ensure these directories exist in 'models/'\n",
    "VARIANTS = [\n",
    "    \"DAPINet\",  # Base model\n",
    "    \"ablation_convex\",  # Trained on Convex\n",
    "    \"ablation_manifold\",  # Trained on Manifold\n",
    "]\n",
    "\n",
    "VARIANT_LABELS = {\n",
    "    \"DAPINet\": \"Base (Full)\",\n",
    "    \"ablation_convex\": \"Trained on Convex\",\n",
    "    \"ablation_manifold\": \"Trained on Manifold\",\n",
    "}\n",
    "\n",
    "# Paths\n",
    "MODELS_DIR = Path(\"models\")\n",
    "RESULTS_DIR = Path(\"results/dataset_ablation\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "REAL_WORLD_DIR = Path(\"datasets/real_world\")\n",
    "ORACLE_PATH = Path(\"results/oracle_ari.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data & Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-28 09:51:33,694 INFO: Loading datasets from datasets\\real_world\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading real-world datasets from: datasets\\real_world\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Datasets: 100%|██████████| 20/20 [00:00<00:00, 513.94it/s]\n",
      "2026-01-28 09:51:33,744 INFO: Loaded 20 datasets.\n"
     ]
    }
   ],
   "source": [
    "# Load real-world datasets\n",
    "print(f\"Loading real-world datasets from: {REAL_WORLD_DIR}\")\n",
    "real_datasets = load_datasets(REAL_WORLD_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-28 09:51:38,915 INFO: Found 5 models in models\\DAPINet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Evaluating: Base (Full)\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-28 09:51:39,158 INFO: Updating Config from checkpoint_fold_1.pth...\n",
      "2026-01-28 09:51:39,187 INFO: Loaded model from models\\DAPINet\\checkpoint_fold_1.pth (Epoch 4, Loss 0.0456)\n",
      "2026-01-28 09:51:39,302 INFO: Updating Config from checkpoint_fold_2.pth...\n",
      "2026-01-28 09:51:39,322 INFO: Loaded model from models\\DAPINet\\checkpoint_fold_2.pth (Epoch 8, Loss 0.0461)\n",
      "2026-01-28 09:51:39,386 INFO: Updating Config from checkpoint_fold_3.pth...\n",
      "2026-01-28 09:51:39,408 INFO: Loaded model from models\\DAPINet\\checkpoint_fold_3.pth (Epoch 9, Loss 0.0513)\n",
      "2026-01-28 09:51:39,468 INFO: Updating Config from checkpoint_fold_4.pth...\n",
      "2026-01-28 09:51:39,486 INFO: Loaded model from models\\DAPINet\\checkpoint_fold_4.pth (Epoch 7, Loss 0.0482)\n",
      "2026-01-28 09:51:39,551 INFO: Updating Config from checkpoint_fold_5.pth...\n",
      "2026-01-28 09:51:39,570 INFO: Loaded model from models\\DAPINet\\checkpoint_fold_5.pth (Epoch 8, Loss 0.0447)\n",
      "Model Inference: 100%|██████████| 20/20 [00:02<00:00,  8.74it/s]\n",
      "2026-01-28 09:51:41,872 INFO: Found 5 models in models\\ablation_convex\n",
      "2026-01-28 09:51:41,961 INFO: Updating Config from checkpoint_fold_1.pth...\n",
      "2026-01-28 09:51:41,980 INFO: Loaded model from models\\ablation_convex\\checkpoint_fold_1.pth (Epoch 4, Loss 0.0499)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results to: results\\dataset_ablation\\real_world_DAPINet.csv\n",
      "\n",
      "============================================================\n",
      "Evaluating: Trained on Convex\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-28 09:51:42,074 INFO: Updating Config from checkpoint_fold_2.pth...\n",
      "2026-01-28 09:51:42,097 INFO: Loaded model from models\\ablation_convex\\checkpoint_fold_2.pth (Epoch 6, Loss 0.0526)\n",
      "2026-01-28 09:51:42,198 INFO: Updating Config from checkpoint_fold_3.pth...\n",
      "2026-01-28 09:51:42,214 INFO: Loaded model from models\\ablation_convex\\checkpoint_fold_3.pth (Epoch 5, Loss 0.0585)\n",
      "2026-01-28 09:51:42,301 INFO: Updating Config from checkpoint_fold_4.pth...\n",
      "2026-01-28 09:51:42,318 INFO: Loaded model from models\\ablation_convex\\checkpoint_fold_4.pth (Epoch 9, Loss 0.0505)\n",
      "2026-01-28 09:51:42,392 INFO: Updating Config from checkpoint_fold_5.pth...\n",
      "2026-01-28 09:51:42,409 INFO: Loaded model from models\\ablation_convex\\checkpoint_fold_5.pth (Epoch 9, Loss 0.0466)\n",
      "Model Inference: 100%|██████████| 20/20 [00:01<00:00, 17.11it/s]\n",
      "2026-01-28 09:51:43,589 INFO: Found 5 models in models\\ablation_manifold\n",
      "2026-01-28 09:51:43,670 INFO: Updating Config from checkpoint_fold_1.pth...\n",
      "2026-01-28 09:51:43,690 INFO: Loaded model from models\\ablation_manifold\\checkpoint_fold_1.pth (Epoch 9, Loss 0.0296)\n",
      "2026-01-28 09:51:43,764 INFO: Updating Config from checkpoint_fold_2.pth...\n",
      "2026-01-28 09:51:43,785 INFO: Loaded model from models\\ablation_manifold\\checkpoint_fold_2.pth (Epoch 8, Loss 0.0260)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results to: results\\dataset_ablation\\real_world_ablation_convex.csv\n",
      "\n",
      "============================================================\n",
      "Evaluating: Trained on Manifold\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-28 09:51:43,870 INFO: Updating Config from checkpoint_fold_3.pth...\n",
      "2026-01-28 09:51:43,888 INFO: Loaded model from models\\ablation_manifold\\checkpoint_fold_3.pth (Epoch 7, Loss 0.0273)\n",
      "2026-01-28 09:51:43,978 INFO: Updating Config from checkpoint_fold_4.pth...\n",
      "2026-01-28 09:51:43,996 INFO: Loaded model from models\\ablation_manifold\\checkpoint_fold_4.pth (Epoch 7, Loss 0.0269)\n",
      "2026-01-28 09:51:44,096 INFO: Updating Config from checkpoint_fold_5.pth...\n",
      "2026-01-28 09:51:44,122 INFO: Loaded model from models\\ablation_manifold\\checkpoint_fold_5.pth (Epoch 6, Loss 0.0278)\n",
      "Model Inference: 100%|██████████| 20/20 [00:01<00:00, 18.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results to: results\\dataset_ablation\\real_world_ablation_manifold.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run inference for each variant\n",
    "results = {}\n",
    "\n",
    "for variant in VARIANTS:\n",
    "    model_path = MODELS_DIR / variant\n",
    "\n",
    "    if not model_path.exists():\n",
    "        print(f\"⚠️  Model directory not found: {model_path}\")\n",
    "        print(f\"    Skipping {variant}...\\n\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"Evaluating: {VARIANT_LABELS[variant]}\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "\n",
    "    df_pred, stats = run_model_inference(model_path, real_datasets)\n",
    "\n",
    "    # Save predictions\n",
    "    output_file = RESULTS_DIR / f\"real_world_{variant}.csv\"\n",
    "    df_pred.to_csv(output_file, index=False)\n",
    "    print(f\"Saved results to: {output_file}\")\n",
    "\n",
    "    results[variant] = df_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparative Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary Statistics on Real-World Datasets:\n",
      "            Variant  Mean Pred ARI  Median Pred ARI  Mean Regret  Median Regret  Near Optimal (<10% Regret)\n",
      "        Base (Full)       0.352790         0.245972     0.048809       0.005170                        90.0\n",
      "  Trained on Convex       0.288083         0.201969     0.113516       0.057164                        70.0\n",
      "Trained on Manifold       0.228758         0.165580     0.172840       0.144153                        50.0\n"
     ]
    }
   ],
   "source": [
    "if not ORACLE_PATH.exists():\n",
    "    print(f\"⚠️ Oracle file not found at {ORACLE_PATH}. Cannot compute regret.\")\n",
    "    oracle_df = None\n",
    "else:\n",
    "    oracle_df = pd.read_csv(ORACLE_PATH).set_index(\"dataset\")\n",
    "\n",
    "summary_stats = []\n",
    "all_regrets = []\n",
    "\n",
    "if oracle_df is not None:\n",
    "    for variant, df_pred in results.items():\n",
    "        algo_cols = [\n",
    "            c\n",
    "            for c in df_pred.columns\n",
    "            if c not in {\"dataset\", \"inference_time_ms\"}\n",
    "        ]\n",
    "        per_dataset = []\n",
    "\n",
    "        for _, row in df_pred.iterrows():\n",
    "            ds_name = row[\"dataset\"]\n",
    "            if ds_name not in oracle_df.index:\n",
    "                continue\n",
    "\n",
    "            # Get predicted algorithm and its true ARI\n",
    "            probs = row[algo_cols].to_numpy(dtype=float)\n",
    "            pred_idx = int(np.argmax(probs))\n",
    "            pred_algo = algo_cols[pred_idx]\n",
    "\n",
    "            true_ari_row = oracle_df.loc[ds_name, algo_cols]\n",
    "            pred_ari = float(true_ari_row[pred_algo])\n",
    "            max_true_ari = float(true_ari_row.max())\n",
    "            regret = max_true_ari - pred_ari\n",
    "\n",
    "            per_dataset.append(\n",
    "                {\n",
    "                    \"dataset\": ds_name,\n",
    "                    \"pred_ari\": pred_ari,\n",
    "                    \"max_true_ari\": max_true_ari,\n",
    "                    \"regret\": regret,\n",
    "                }\n",
    "            )\n",
    "\n",
    "            all_regrets.append({\"Variant\": VARIANT_LABELS[variant], \"Regret\": regret})\n",
    "\n",
    "        ds_df = pd.DataFrame(per_dataset)\n",
    "\n",
    "        summary_stats.append(\n",
    "            {\n",
    "                \"Variant\": VARIANT_LABELS[variant],\n",
    "                \"Mean Pred ARI\": ds_df[\"pred_ari\"].mean(),\n",
    "                \"Median Pred ARI\": ds_df[\"pred_ari\"].median(),\n",
    "                \"Mean Regret\": ds_df[\"regret\"].mean(),\n",
    "                \"Median Regret\": ds_df[\"regret\"].median(),\n",
    "                \"Near Optimal (<10% Regret)\": (ds_df[\"regret\"] <= 0.10).mean() * 100,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    summary_df = pd.DataFrame(summary_stats)\n",
    "    print(\"\\nSummary Statistics on Real-World Datasets:\")\n",
    "    print(summary_df.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
